{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DistilBERT — классификация паттернов и ролей участников\n",
        "\n",
        "## Цель\n",
        "Обучить модель классификации паттернов активности и ролей участников open-source проектов на основе:\n",
        "- **Текстов комментариев** пользователя (агрегированные)\n",
        "- **Метрик активности**: PRs authored, PRs reviewed, comments count\n",
        "\n",
        "## Данные\n",
        "- **Источники**: microsoft/vscode + chaoss/augur\n",
        "- **Объём**: 7000+ комментариев\n",
        "- **Вход модели**: `[PRs:5 Reviews:12 Comments:30] Текст комментариев...`\n",
        "\n",
        "## Классы (7 паттернов → роли)\n",
        "| Паттерн | Роли |\n",
        "|---------|------|\n",
        "| Пассивного потребления | Lurker, Passive user, Rare contributor |\n",
        "| Инициации обратной связи | Bug reporter, Coordinator |\n",
        "| Периферийного участия | Peripheral developer, Nomad Coder |\n",
        "| Активного соисполнительства | Bug fixer, Active developer, Code Warrior |\n",
        "| Кураторства и управления | Project steward, Coordinator, Progress controller |\n",
        "| Лидерства и наставничества | Project leader, Core member, Core developer |\n",
        "| Социального влияния | Project Rockstar |\n",
        "\n",
        "## Алгоритм\n",
        "- **Модель**: DistilBERT-base-multilingual-cased\n",
        "- **Вход**: метрики активности + тексты комментариев\n",
        "- **Выход**: паттерн участия\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Устанавливаем зависимости (Colab-friendly)\n",
        "import sys, subprocess, pkg_resources, os\n",
        "from pathlib import Path\n",
        "try: pkg_resources.get_distribution(\"accelerate>=0.26.0\")\n",
        "except: subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"accelerate>=0.26.0\"])\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    !git clone https://github.com/elenagernichenko/mcp-analyzer.git 2>/dev/null || true\n",
        "    %cd /content/mcp-analyzer\n",
        "\n",
        "PROJECT_ROOT = Path(\".\").resolve()\n",
        "PR_SAMPLES = PROJECT_ROOT / \"data\" / \"pr_samples_multi.json\"  # 7000+ комментариев из 2 репозиториев\n",
        "print(\"DATA:\", PR_SAMPLES) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/spectreofoblivion/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Optional: !pip install transformers datasets scikit-learn pandas\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "device = \"cuda\" if (os.getenv(\"CUDA_VISIBLE_DEVICES\") not in [None, \"\"] and os.environ.get(\"CUDA_VISIBLE_DEVICES\") != \"-1\") else \"cpu\"\n",
        "print(\"device:\", device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "samples: 10\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>User jeroen authored 1 PRs, reviewed 0 PRs in ...</td>\n",
              "      <td>Периферийного участия</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>User andrey-khropov authored 0 PRs, reviewed 5...</td>\n",
              "      <td>Кураторства и управления</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>User topepo authored 0 PRs, reviewed 1 PRs in ...</td>\n",
              "      <td>Периферийного участия</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>User isaigordeev authored 1 PRs, reviewed 0 PR...</td>\n",
              "      <td>Периферийного участия</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>User Shersula authored 1 PRs, reviewed 0 PRs i...</td>\n",
              "      <td>Периферийного участия</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                     label\n",
              "0  User jeroen authored 1 PRs, reviewed 0 PRs in ...     Периферийного участия\n",
              "1  User andrey-khropov authored 0 PRs, reviewed 5...  Кураторства и управления\n",
              "2  User topepo authored 0 PRs, reviewed 1 PRs in ...     Периферийного участия\n",
              "3  User isaigordeev authored 1 PRs, reviewed 0 PR...     Периферийного участия\n",
              "4  User Shersula authored 1 PRs, reviewed 0 PRs i...     Периферийного участия"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Загружаем PR и агрегируем данные ПО ПОЛЬЗОВАТЕЛЯМ (метрики + тексты)\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "with open(PR_SAMPLES, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "prs = data.get(\"prs\", [])\n",
        "\n",
        "# Собираем статистику и комментарии по каждому пользователю\n",
        "user_stats = defaultdict(lambda: {\"authored\": 0, \"reviewed\": 0, \"comments\": [], \"prs_participated\": set()})\n",
        "\n",
        "for pr in prs:\n",
        "    author = pr.get(\"author\", \"\")\n",
        "    if author: user_stats[author][\"authored\"] += 1\n",
        "    \n",
        "    for c in pr.get(\"comments\", []):\n",
        "        user, body = c.get(\"user\", \"\"), c.get(\"body\", \"\").strip()\n",
        "        if user and body and \"bot\" not in user.lower():\n",
        "            user_stats[user][\"comments\"].append(body)\n",
        "            user_stats[user][\"prs_participated\"].add(pr.get(\"number\"))\n",
        "\n",
        "# Классификация паттерна по метрикам активности + содержанию комментариев\n",
        "def classify_user(stats: dict, comments_text: str) -> str:\n",
        "    authored, reviewed = stats[\"authored\"], len(stats[\"prs_participated\"]) - stats[\"authored\"]\n",
        "    n_comments = len(stats[\"comments\"])\n",
        "    total_activity = authored + reviewed + n_comments\n",
        "    text_lower = comments_text.lower()\n",
        "    \n",
        "    # По активности и содержанию\n",
        "    if total_activity < 3: return \"Пассивного потребления\"\n",
        "    if authored == 0 and n_comments > 0 and any(k in text_lower for k in [\"bug\", \"issue\", \"error\", \"?\"]): \n",
        "        return \"Инициации обратной связи\"\n",
        "    if reviewed > authored * 2 or any(k in text_lower for k in [\"lgtm\", \"approved\", \"merge\", \"please fix\"]):\n",
        "        return \"Кураторства и управления\"\n",
        "    if authored > 10 and n_comments > 20 and any(k in text_lower for k in [\"architecture\", \"design\", \"explain\"]):\n",
        "        return \"Лидерства и наставничества\"\n",
        "    if any(k in text_lower for k in [\"community\", \"team\", \"everyone\", \"milestone\", \"release\"]):\n",
        "        return \"Социального влияния\"\n",
        "    if authored > 3 or any(k in text_lower for k in [\"fixed\", \"implemented\", \"added\", \"commit\"]):\n",
        "        return \"Активного соисполнительства\"\n",
        "    return \"Периферийного участия\"\n",
        "\n",
        "# Формируем датасет: один пользователь = один пример\n",
        "rows = []\n",
        "for user, stats in user_stats.items():\n",
        "    if len(stats[\"comments\"]) < 2: continue  # минимум 2 комментария\n",
        "    \n",
        "    # Метрики активности\n",
        "    authored, reviewed = stats[\"authored\"], len(stats[\"prs_participated\"]) - stats[\"authored\"]\n",
        "    n_comments = len(stats[\"comments\"])\n",
        "    \n",
        "    # Текст: метрики + комментарии (до 400 символов)\n",
        "    comments_text = \" | \".join(stats[\"comments\"])[:400]\n",
        "    text = f\"[PRs:{authored} Reviews:{max(0,reviewed)} Comments:{n_comments}] {comments_text}\"\n",
        "    \n",
        "    label = classify_user(stats, comments_text)\n",
        "    rows.append({\"text\": text, \"label\": label, \"user\": user})\n",
        "\n",
        "label_counts = Counter(r[\"label\"] for r in rows)\n",
        "print(f\"Пользователей: {len(rows)}, Классов: {len(label_counts)}\")\n",
        "print(\"Распределение:\", dict(label_counts.most_common()))\n",
        "pd.DataFrame(rows).head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 7/7 [00:00<00:00, 707.20 examples/s]\n",
            "Map: 100%|██████████| 3/3 [00:00<00:00, 848.53 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Подготовка датасета для обучения\n",
        "unique_labels = sorted(set(r[\"label\"] for r in rows))\n",
        "label2id = {lbl: i for i, lbl in enumerate(unique_labels)}\n",
        "id2label = {i: lbl for lbl, i in label2id.items()}\n",
        "\n",
        "df = pd.DataFrame(rows)[[\"text\", \"label\"]]  # только нужные колонки\n",
        "counts = df[\"label\"].value_counts()\n",
        "print(\"Классы:\", dict(counts))\n",
        "\n",
        "# Stratified split (если возможно)\n",
        "try:\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
        "except:\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df, preserve_index=False)\n",
        "test_ds = Dataset.from_pandas(test_df, preserve_index=False)\n",
        "\n",
        "model_name = \"distilbert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def preprocess(batch):\n",
        "    enc = tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
        "    enc[\"labels\"] = [label2id[lbl] for lbl in batch[\"label\"]]\n",
        "    return enc\n",
        "\n",
        "train_ds = train_ds.map(preprocess, batched=True, remove_columns=[\"text\", \"label\"])\n",
        "test_ds = test_ds.map(preprocess, batched=True, remove_columns=[\"text\", \"label\"])\n",
        "datasets = DatasetDict({\"train\": train_ds, \"test\": test_ds})\n",
        "print(f\"Train: {len(train_ds)}, Test: {len(test_ds)}, Classes: {len(unique_labels)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipykernel_22448/1337417747.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "/home/spectreofoblivion/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 00:21, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.635900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Обучение DistilBERT для классификации паттернов\n",
        "# Алгоритм: fine-tuning предобученного DistilBERT с новым classification head\n",
        "# Оптимизатор: AdamW (lr=3e-5, weight_decay=0.01)\n",
        "# Scheduler: linear warmup + decay (по умолчанию в Trainer)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(unique_labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "# Гиперпараметры для полноценного обучения\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./roles-checkpoints\",\n",
        "    num_train_epochs=5,  # больше эпох для лучшей сходимости\n",
        "    per_device_train_batch_size=16,  # увеличенный batch для стабильности градиентов\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=3e-5,  # стандартный LR для fine-tuning BERT\n",
        "    weight_decay=0.01,  # L2 регуляризация\n",
        "    warmup_ratio=0.1,  # 10% шагов на warmup\n",
        "    logging_steps=50,\n",
        "    remove_unused_columns=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=datasets[\"train\"],\n",
        "    eval_dataset=datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(f\"Training on {len(datasets['train'])} samples, evaluating on {len(datasets['test'])} samples\")\n",
        "print(f\"Classes: {len(unique_labels)}, Epochs: {args.num_train_epochs}, Batch: {args.per_device_train_batch_size}\")\n",
        "trainer.train();\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/spectreofoblivion/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test size: 3\n",
            "                             precision    recall  f1-score   support\n",
            "\n",
            "Активного соисполнительства       0.00      0.00      0.00         1\n",
            "   Кураторства и управления       0.00      0.00      0.00         0\n",
            "      Периферийного участия       0.67      1.00      0.80         2\n",
            "\n",
            "                   accuracy                           0.67         3\n",
            "                  macro avg       0.22      0.33      0.27         3\n",
            "               weighted avg       0.44      0.67      0.53         3\n",
            "\n",
            "Confusion matrix:\n",
            " [[0 0 1]\n",
            " [0 0 0]\n",
            " [0 0 2]]\n"
          ]
        },
        {
          "ename": "SafetensorError",
          "evalue": "Error while serializing: I/O error: No space left on device (os error 28)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSafetensorError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(true_labels, pred_labels, labels\u001b[38;5;241m=\u001b[39mlabels_list, target_names\u001b[38;5;241m=\u001b[39mtarget_names, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion matrix:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, confusion_matrix(true_labels, pred_labels, labels\u001b[38;5;241m=\u001b[39mlabels_list))\n\u001b[0;32m---> 15\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistilbert-finetune-roles\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-finetune-roles\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:4173\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   4168\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m   4170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[1;32m   4171\u001b[0m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[1;32m   4172\u001b[0m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[0;32m-> 4173\u001b[0m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4175\u001b[0m     save_function(shard, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, shard_file))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/safetensors/torch.py:307\u001b[0m, in \u001b[0;36msave_file\u001b[0;34m(tensors, filename, metadata)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_file\u001b[39m(\n\u001b[1;32m    277\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    278\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    279\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    280\u001b[0m ):\n\u001b[1;32m    281\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m     \u001b[43mserialize_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mSafetensorError\u001b[0m: Error while serializing: I/O error: No space left on device (os error 28)"
          ]
        }
      ],
      "source": [
        "# Оценка модели по ключевым метрикам\n",
        "# - Accuracy: общая доля правильных предсказаний\n",
        "# - Precision: точность (TP / (TP + FP)) — сколько предсказанных верны\n",
        "# - Recall: полнота (TP / (TP + FN)) — сколько реальных найдено\n",
        "# - F1-score: гармоническое среднее precision и recall\n",
        "# - Macro F1: среднее F1 по всем классам (важно при дисбалансе)\n",
        "# - Weighted F1: F1 с весами по размеру класса\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "preds = trainer.predict(datasets[\"test\"])\n",
        "logits = preds.predictions\n",
        "pred_labels = np.argmax(logits, axis=1)\n",
        "true_labels = preds.label_ids\n",
        "\n",
        "labels_list = sorted(id2label)\n",
        "target_names = [id2label[i] for i in labels_list]\n",
        "\n",
        "# Основные метрики\n",
        "accuracy = accuracy_score(true_labels, pred_labels)\n",
        "macro_f1 = f1_score(true_labels, pred_labels, average=\"macro\", zero_division=0)\n",
        "weighted_f1 = f1_score(true_labels, pred_labels, average=\"weighted\", zero_division=0)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"РЕЗУЛЬТАТЫ ДООБУЧЕНИЯ DistilBERT\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Test size: {len(true_labels)} samples\")\n",
        "print(f\"Classes: {len(unique_labels)}\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Accuracy:    {accuracy:.2%}\")\n",
        "print(f\"Macro F1:    {macro_f1:.2%}\")\n",
        "print(f\"Weighted F1: {weighted_f1:.2%}\")\n",
        "print(\"-\" * 60)\n",
        "print(\"\\nПодробный отчет по классам:\")\n",
        "print(classification_report(true_labels, pred_labels, labels=labels_list, target_names=target_names, zero_division=0))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(true_labels, pred_labels, labels=labels_list))\n",
        "\n",
        "# Сохранение модели\n",
        "model.save_pretrained(\"distilbert-finetune-roles\")\n",
        "tokenizer.save_pretrained(\"distilbert-finetune-roles\")\n",
        "print(\"\\nМодель сохранена в distilbert-finetune-roles/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Функция определения роли по паттерну\n",
        "PATTERN_TO_ROLES = {\n",
        "    \"Пассивного потребления\": [\"Lurker\", \"Passive user\", \"Rare contributor\"],\n",
        "    \"Инициации обратной связи\": [\"Bug reporter\", \"Coordinator\"],\n",
        "    \"Периферийного участия\": [\"Peripheral developer\", \"Nomad Coder\", \"Independent\"],\n",
        "    \"Активного соисполнительства\": [\"Bug fixer\", \"Active developer\", \"Code Warrior\"],\n",
        "    \"Кураторства и управления\": [\"Project steward\", \"Coordinator\", \"Progress controller\"],\n",
        "    \"Лидерства и наставничества\": [\"Project leader\", \"Core member\", \"Core developer\"],\n",
        "    \"Социального влияния\": [\"Project Rockstar\"],\n",
        "}\n",
        "\n",
        "def predict_user_role(prs_authored: int, reviews: int, comments: list[str]) -> tuple[str, str, float]:\n",
        "    \"\"\"Предсказание паттерна и роли пользователя.\"\"\"\n",
        "    n_comments = len(comments)\n",
        "    comments_text = \" | \".join(comments)[:400]\n",
        "    text = f\"[PRs:{prs_authored} Reviews:{reviews} Comments:{n_comments}] {comments_text}\"\n",
        "    \n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    probs = torch.softmax(outputs.logits, dim=1)\n",
        "    pred_id = torch.argmax(probs, dim=1).item()\n",
        "    \n",
        "    pattern = model.config.id2label[pred_id]\n",
        "    role = PATTERN_TO_ROLES.get(pattern, [\"Unknown\"])[0]\n",
        "    confidence = probs[0][pred_id].item()\n",
        "    return pattern, role, confidence\n",
        "\n",
        "# Примеры использования\n",
        "import torch\n",
        "examples = [\n",
        "    (0, 0, [\"Thanks!\", \"Nice work\"]),\n",
        "    (5, 2, [\"Fixed the bug\", \"Implemented new feature\", \"Added tests\"]),\n",
        "    (0, 10, [\"LGTM\", \"Please fix this\", \"Approved\", \"Merge when ready\"]),\n",
        "    (0, 1, [\"This causes a crash\", \"Bug: doesn't work on Windows\"]),\n",
        "]\n",
        "\n",
        "print(\"Примеры предсказаний:\")\n",
        "for prs, reviews, comments in examples:\n",
        "    pattern, role, conf = predict_user_role(prs, reviews, comments)\n",
        "    print(f\"  PRs:{prs} Reviews:{reviews} → {pattern} ({role}) [{conf:.0%}]\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
