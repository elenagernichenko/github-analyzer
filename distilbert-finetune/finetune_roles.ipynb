{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DistilBERT — 7 паттернов + роли\n",
        "\n",
        "## Данные\n",
        "- Источник: microsoft/vscode (цель 7000 PR, файл data/pr_samples_vscode_7000.json)\n",
        "- Вход: `[PRs:x Reviews:y Comments:z] тексты комментариев пользователя`\n",
        "\n",
        "## Паттерны → роли\n",
        "| Паттерн | Роли |\n",
        "|---------|------|\n",
        "| Пассивного потребления | Lurker, Passive user, Rare contributor |\n",
        "| Инициации обратной связи | Bug reporter, Coordinator |\n",
        "| Периферийного участия | Peripheral developer, Nomad Coder, Independent |\n",
        "| Активного соисполнительства | Bug fixer, Active developer, Code Warrior |\n",
        "| Кураторства и управления | Project steward, Coordinator, Progress controller |\n",
        "| Лидерства и наставничества | Project leader, Core member, Core developer |\n",
        "| Социального влияния | Project Rockstar |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup (Colab-friendly)\n",
        "import sys, subprocess, pkg_resources, os\n",
        "from pathlib import Path\n",
        "try: pkg_resources.get_distribution('accelerate>=0.26.0')\n",
        "except: subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'accelerate>=0.26.0'])\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    !git clone https://github.com/elenagernichenko/github-analyzer.git 2>/dev/null || true\n",
        "    %cd /content/github-analyzer\n",
        "\n",
        "PROJECT_ROOT = Path('.').resolve()\n",
        "PR_SAMPLES = PROJECT_ROOT / 'data' / 'pr_samples_vscode_7000.json'\n",
        "print('DATA:', PR_SAMPLES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('device:', device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "with open(PR_SAMPLES, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "prs = data.get('prs', [])\n",
        "\n",
        "user_stats = defaultdict(lambda: {'authored': 0, 'comments': [], 'prs': set()})\n",
        "for pr in prs:\n",
        "    author = pr.get('author', '')\n",
        "    if author: user_stats[author]['authored'] += 1\n",
        "    for c in pr.get('comments', []):\n",
        "        user, body = c.get('user', ''), c.get('body', '').strip()\n",
        "        if user and body and 'bot' not in user.lower() and 'copilot' not in user.lower():\n",
        "            user_stats[user]['comments'].append(body)\n",
        "            user_stats[user]['prs'].add(pr.get('number'))\n",
        "\n",
        "print('Users total:', len(user_stats))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heuristic classification (7 паттернов)\n",
        "def classify(text: str, authored: int, reviewed: int, n_comments: int) -> str:\n",
        "    t = text.lower()\n",
        "    total = authored + reviewed + n_comments\n",
        "    if total < 2: return 'Пассивного потребления'\n",
        "    if authored == 0 and n_comments > 0 and any(k in t for k in ['bug','issue','error','crash','fail']):\n",
        "        return 'Инициации обратной связи'\n",
        "    if any(k in t for k in ['community','team','milestone','release']):\n",
        "        return 'Социального влияния'\n",
        "    if authored > 10 and n_comments > 20 and any(k in t for k in ['architecture','design','explain','doc']):\n",
        "        return 'Лидерства и наставничества'\n",
        "    if reviewed > authored*2 or any(k in t for k in ['lgtm','approved','merge','please fix','please add']):\n",
        "        return 'Кураторства и управления'\n",
        "    if authored > 3 or any(k in t for k in ['fixed','implemented','added','commit','updated']):\n",
        "        return 'Активного соисполнительства'\n",
        "    return 'Периферийного участия'\n",
        "\n",
        "rows = []\n",
        "for user, stats in user_stats.items():\n",
        "    if len(stats['comments']) < 2: continue\n",
        "    authored = stats['authored']\n",
        "    reviewed = max(0, len(stats['prs']) - authored)\n",
        "    n_comments = len(stats['comments'])\n",
        "    comments_text = ' | '.join(stats['comments'])[:400]\n",
        "    label = classify(comments_text, authored, reviewed, n_comments)\n",
        "    text = f'[PRs:{authored} Reviews:{reviewed} Comments:{n_comments}] {comments_text}'\n",
        "    rows.append({'text': text, 'label': label})\n",
        "\n",
        "print('Users with >=2 comments:', len(rows))\n",
        "print('Distribution:', dict(Counter(r['label'] for r in rows)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset + class weights\n",
        "unique_labels = sorted(set(r['label'] for r in rows))\n",
        "label2id = {lbl:i for i,lbl in enumerate(unique_labels)}\n",
        "id2label = {i:lbl for lbl,i in label2id.items()}\n",
        "df = pd.DataFrame(rows)\n",
        "counts = df['label'].value_counts()\n",
        "class_weights = {label2id[lbl]: len(df)/(len(counts)*cnt) for lbl,cnt in counts.items()}\n",
        "print('Weights:', {id2label[i]: f'{w:.2f}' for i,w in class_weights.items()})\n",
        "\n",
        "# Stratified split; если редкий класс <2 — без stratify\n",
        "if counts.min() < 2:\n",
        "    print(\"Stratify disabled: rare class has <2 samples\")\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
        "else:\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
        "\n",
        "def preprocess(batch):\n",
        "    enc = tokenizer(batch['text'], truncation=True, padding='max_length', max_length=256)\n",
        "    enc['labels'] = [label2id[lbl] for lbl in batch['label']]\n",
        "    return enc\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df, preserve_index=False).map(preprocess, batched=True, remove_columns=['text','label'])\n",
        "test_ds  = Dataset.from_pandas(test_df,  preserve_index=False).map(preprocess, batched=True, remove_columns=['text','label'])\n",
        "print(f'Train: {len(train_ds)}, Test: {len(test_ds)}, Classes: {len(unique_labels)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Weighted Trainer\n",
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, class_weights, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.w = torch.tensor([class_weights[i] for i in range(len(class_weights))], dtype=torch.float)\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop('labels')\n",
        "        outputs = model(**inputs)\n",
        "        loss = torch.nn.CrossEntropyLoss(weight=self.w.to(outputs.logits.device))(outputs.logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-multilingual-cased', num_labels=len(unique_labels), id2label=id2label, label2id=label2id)\n",
        "args = TrainingArguments(output_dir='./ckpt', num_train_epochs=10, per_device_train_batch_size=4, learning_rate=2e-5, weight_decay=0.01, warmup_ratio=0.1, logging_steps=10, report_to='none')\n",
        "trainer = WeightedTrainer(class_weights=class_weights, model=model, args=args, train_dataset=train_ds, eval_dataset=test_ds, tokenizer=tokenizer)\n",
        "print(f'Training {len(train_ds)} samples, {len(unique_labels)} classes')\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (old cell - skip, use Cell 8 below)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation (fixed)\n",
        "preds = trainer.predict(test_ds)\n",
        "pred_labels = np.argmax(preds.predictions, axis=1)\n",
        "true_labels = preds.label_ids\n",
        "\n",
        "print('=' * 60)\n",
        "print('RESULTS (class weights)')\n",
        "print('=' * 60)\n",
        "print(f'Accuracy: {accuracy_score(true_labels, pred_labels):.2%}')\n",
        "print(f'Macro F1: {f1_score(true_labels, pred_labels, average=\"macro\", zero_division=0):.2%}')\n",
        "print('\\n' + classification_report(true_labels, pred_labels, target_names=[id2label[i] for i in sorted(id2label)], zero_division=0))\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(true_labels, pred_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pattern -> role mapping and predict helper\n",
        "PATTERN_TO_ROLES = {\n",
        "    'Пассивного потребления': ['Lurker','Passive user','Rare contributor'],\n",
        "    'Инициации обратной связи': ['Bug reporter','Coordinator'],\n",
        "    'Периферийного участия': ['Peripheral developer','Nomad Coder','Independent'],\n",
        "    'Активного соисполнительства': ['Bug fixer','Active developer','Code Warrior'],\n",
        "    'Кураторства и управления': ['Project steward','Coordinator','Progress controller'],\n",
        "    'Лидерства и наставничества': ['Project leader','Core member','Core developer'],\n",
        "    'Социального влияния': ['Project Rockstar'],\n",
        "}\n",
        "\n",
        "def predict_user(prs_authored:int, reviews:int, comments:list[str]):\n",
        "    text_comments = ' | '.join(comments)[:400]\n",
        "    text = f'[PRs:{prs_authored} Reviews:{reviews} Comments:{len(comments)}] {text_comments}'\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=256)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    probs = torch.softmax(outputs.logits, dim=1)\n",
        "    pred_id = torch.argmax(probs, dim=1).item()\n",
        "    pattern = model.config.id2label[pred_id]\n",
        "    role = PATTERN_TO_ROLES.get(pattern, ['Unknown'])[0]\n",
        "    conf = probs[0][pred_id].item()\n",
        "    return pattern, role, conf\n",
        "\n",
        "print('Примеры предсказаний:')\n",
        "for example in [\n",
        "    (0,0,['Thanks!','Nice work']),\n",
        "    (5,2,['Fixed bug','Implemented feature','Added tests']),\n",
        "    (0,5,['LGTM','Please fix this','Approved']),\n",
        "    (0,1,['Bug: crashes on start','Error on Windows']),\n",
        "]:\n",
        "    p,r,c = predict_user(*example)\n",
        "    print(f'  [{c:.0%}] {p} -> {r}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
